# -*- coding: utf-8 -*-
"""fds_hw1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/148E3gIOlS8FQI3N_ocwV5IS2Jyn40xIQ
"""

import numpy as np
import matplotlib.pyplot as plt

# Problem 1:
rbvals = [(1,1),(2,2),(5,5),(1,5),(5,1),(5,20)]
rbvals04 = [(5,60),(3,10)] # vals for r and b that give max slope at x=0.4

'''
Experiment with different values of r and b to determine a pair of (not too 
large) positive integers r and b so that the function f (x) has the maximum 
slope around x = 0.4.
'''

r,b=rbvals04[0]
x = np.linspace(0,1); f = 1-(1-x**r)**b; 
# max slope at x=0.4 for r=5, b=60 or r=3, b=10
plt.title('r=5,b=60'); plt.plot(x,f); plt.show() 

'''
Plot the function f (x) = 1−(1−x r ) b for 
(r, b) ∈ {(1, 1), (2, 2), (5, 5), (1, 5), (5, 1), (5, 20)} and x ∈ [0, 1]. 
Plot the six functions obtained for the different values of r and b as subplots 
of the same figure.
'''

for val in rbvals:
    r,b=val
    f = 1-(1-x**r)**b  
    plt.title("r=%d,b=%d"% (r, b))
    plt.plot(x,f); plt.show()

# Problem 2

'''
N = 10**9   Total number of people
D = 1000    Duration of observation
H = 10**5   Number of hotels
P = 0.01    Probability person i will stay at a hotel
'''

# Part a)

N = 10**9   
D = 2000   
H = 10**5   
P = 0.01    

# nmbr of Evil events = pairs of people * pairs of days * prob of Evil event
nEvil = N**2/2 * D**2/2 * (P**2/H)**2

print(nEvil) # 1 mil ppl

# Part b)

N = 2*10**9   
D = 1000   
H = 2*10**5   
P = 0.01    

# nmbr of Evil events = pairs of people * pairs of days * prob of Evil event
nEvil = N**2/2 * D**2/2 * (P**2/H)**2

print(nEvil) # 250,000 ppl

# Part c)

N = 10**9   
D = 1000   
H = 10**5   
P = 0.01    

# nmbr of Evil events = pairs of people * pairs of days * prob of Evil event
nEvil = N**2/2 * D**2/2 * (P**3/H)**2

print(nEvil) # 25 ppl

"""
%%shell 
apt-get install openjdk-8-jdk-headless -qq > /dev/null
rm -f spark*.tgz*
wget -q https://www-eu.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz
tar xf spark-2.4.0-bin-hadoop2.7.tgz
pip install -q findspark

%%shell
# WARNING!: some rm -f below, read before running
rm -f ml-latest-small.zip
rm -rf ml-latest-small
rm -f links.csv tags.csv ratings.csv README.txt movies.csv
wget -q http://files.grouplens.org/datasets/movielens/ml-latest-small.zip -O ml-latest-small.zip
unzip -j ml-latest-small.zip
rm -f ml-latest-small.zip
ls -l
"""

# SET PATHS
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.0-bin-hadoop2.7"

# INITIALIZE FINDSPARK
import findspark
findspark.init()

# CREATE CONTEXT
import pyspark
sc = pyspark.SparkContext(appName="hw1")

# Problem 4

links = sc.textFile("links.csv")
tags = sc.textFile("tags.csv")
ratings = sc.textFile("ratings.csv")
movies = sc.textFile("movies.csv")
files = {"links":links, "tags":tags, "ratings":ratings, "movies":movies}

def remove_header(itr_index, itr):
    return iter(list(itr)[1:]) if itr_index == 0 else itr

def clean_commas(item):
#     item = item.strip().lower().split(', ')
    if '"' in item:
        item = item.split('"')
        item[1]=item[1].replace(',','')
        item=''.join(item)
    return item


def clean(entry):
    return entry.strip().lower().split(',')

  
for filename in files:
    print(files[filename].take(50))
    files[filename] = files[filename].mapPartitionsWithIndex(remove_header)
    print(files[filename].take(50),"\n")

# 1
def avg_num_of_ratings_per_movie(rdd):
    rdd = rdd.map(clean).map(lambda entry: (entry[1],1)).reduceByKey(lambda x,y: x+y)
    n = rdd.count()
    rdd = rdd.map(lambda entry: (1,entry[1])).reduceByKey(lambda x,y: x+y)
    return rdd.first()[1]/n

# 2
def avg_rating_per_genre(genreRdd, ratingRdd):
    rddG = genreRdd.map(clean_commas).map(clean).flatMap(lambda entry: [(entry[0],genre) for genre in entry[2].split('|')])
    rddR = ratingRdd.map(clean).map(lambda entry: (entry[1],entry[2]))
    temp_rddR = rddR.map(lambda entry: (entry[0],1)).reduceByKey(lambda x,y: x+y)
    temp_rdd = temp_rddR.join(rddG).map(lambda entry: (entry[1][1],entry[1][0])).reduceByKey(lambda x,y: x+y)
    rdd = rddR.join(rddG).map(lambda entry: (entry[1][1],float(entry[1][0]))).reduceByKey(lambda x,y: x+y)
    rdd = rdd.join(temp_rdd).map(lambda entry: (entry[0],entry[1][0]/entry[1][1]))
    return rdd.collect()

# 3
def top_three_movies_per_genre(genreRdd, ratingRdd):
    rddG = genreRdd.map(clean_commas).map(clean).flatMap(lambda entry: [(entry[0], (entry[1],genre)) for genre in entry[2].split('|')]) # rddG:      (film_id, (title, genre))
    rddR = ratingRdd.map(clean).map(lambda entry: (entry[1],float(entry[2])))                                                           # rddR:      (film_id, individual_rat) 
    rddR_nums = rddR.map(lambda entry: (entry[0],1)).reduceByKey(lambda x,y: x+y)                                                       # rddR_nums: (film_id, total_num_of_ratings),
    rddR = rddR_nums.join(rddR.reduceByKey(lambda x,y: x+y)).map(lambda entry: (entry[0],entry[1][1]/entry[1][0]))                      # rddR:      (film_id, avg_rat)
    rdd = rddG.join(rddR).map(lambda x: (x[1][0][1], (x[1][0][0],x[1][1])))                                                             # rdd:       (genre, (title, avg_rat))
    rddG = rddG.map(lambda entry: (entry[1][1],1)).reduceByKey(lambda x,y: x+y)
    top3s=[]
    for genre in rddG.collect():
        top3s.append((genre[0],rdd.filter(lambda x: x[0] == genre[0]).map(lambda entry: entry[1]).takeOrdered(3, key = lambda x: -x[1])))
    return top3s

# 4
def top_ten_users_by_ratings(ratingRdd):
    topten = ratingRdd.map(clean).map(lambda entry: (entry[0],1)).reduceByKey(lambda x,y: x+y).takeOrdered(10, key = lambda x: -x[1])
    return topten

# 5 
def top_ten_pairs_of_users_by_movies(rdd):
    # take (user, all movies watched by this user) and (movie, users who watched this movie) and by joining them generate (user, all users who have watched movies seen by this user)
    # then flatmap that to get pairs of users per matching movie for each user and then reduce by key (user pair in this case) to count how many times each pair appears in the rdd
    # now each user pair will have a matching index associated with it
    # just takeOrdered 10 when in descending order
    users = rdd.map(clean).map(lambda x: x[0]).distinct() # user id's (unique)
    rdd_user_movies = rdd.map(clean).map(lambda entry: (entry[0],entry[1])).groupByKey().mapValues(tuple) #(user, (movies))
    rdd_movie_users = rdd.map(clean).map(lambda entry: (entry[1],entry[0])).groupByKey().mapValues(tuple) #(movie, (users))
    rdd_movie_user = rdd_user_movies.flatMap(lambda entry: [(movie,entry[0]) for movie in entry[1]])      #(movie, user)
    rdd = rdd_movie_user.join(rdd_movie_users).map(lambda entry: entry[1])                                #(movie, user).join(movie, (users)) -> (user,(users))
    rdd = rdd.flatMap(lambda entry: [(tuple(sorted([int(entry[0]),int(user)])),1) for user in entry[1]]).filter(lambda x:x[0][0]!=x[0][1]).reduceByKey(lambda x,y: x+y)
    #(user, (users)) -> ((user1, user2),1) -> ((user1, user2),n)
    topten = rdd.takeOrdered(10, key = lambda x: -x[1])
    return topten

if __name__=="__main__":
#     print(avg_num_of_ratings_per_movie(files["ratings"]))
#     print(avg_rating_per_genre(files["movies"],files["ratings"]))
#     print(top_three_movies_per_genre(files["movies"],files["ratings"]))
#     print(top_ten_users_by_ratings(files["ratings"]))
    print(top_ten_pairs_of_users_by_movies(files["ratings"]))

